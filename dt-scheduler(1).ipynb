{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":219664,"sourceType":"modelInstanceVersion","modelInstanceId":187338,"modelId":209416}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"gpuType":"T4","name":"resnet101-34-kd","provenance":[]},"papermill":{"default_parameters":{},"duration":7227.877548,"end_time":"2024-12-28T17:03:45.676396","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-28T15:03:17.798848","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorboard tensorboardX -q\nimport math\nimport os\nimport time\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport getpass\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nimport sys\n\nfrom __future__ import absolute_import\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom torchvision.models import ResNet\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet18, resnet101, resnet34\nfrom torch.utils.data import DataLoader\nimport math\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:05:59.784667Z","iopub.execute_input":"2025-01-24T09:05:59.784877Z","iopub.status.idle":"2025-01-24T09:06:16.008954Z","shell.execute_reply.started":"2025-01-24T09:05:59.784856Z","shell.execute_reply":"2025-01-24T09:06:16.008270Z"},"papermill":{"duration":4.486292,"end_time":"2024-12-28T15:03:24.743169","exception":false,"start_time":"2024-12-28T15:03:20.256877","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(key=\"9d5a8aab3348b03e43147ae4735979a983a3e7b0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:06:16.009794Z","iopub.execute_input":"2025-01-24T09:06:16.010101Z","iopub.status.idle":"2025-01-24T09:06:22.891104Z","shell.execute_reply.started":"2025-01-24T09:06:16.010072Z","shell.execute_reply":"2025-01-24T09:06:22.890255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dtkd_losses = []\ndtkd_accuracies = []\nour_losses = []\nour_accuracies = []\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, is_last=False):\n        super(BasicBlock, self).__init__()\n        self.is_last = is_last\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_planes,\n                    self.expansion * planes,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(self.expansion * planes),\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        preact = out\n        out = F.relu(out)\n        if self.is_last:\n            return out, preact\n        else:\n            return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1, is_last=False):\n        super(Bottleneck, self).__init__()\n        self.is_last = is_last\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(\n            planes, self.expansion * planes, kernel_size=1, bias=False\n        )\n        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_planes,\n                    self.expansion * planes,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(self.expansion * planes),\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        preact = out\n        out = F.relu(out)\n        if self.is_last:\n            return out, preact\n        else:\n            return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n        self.stage_channels = [256, 512, 1024, 2048]\n\n    def get_feat_modules(self):\n        feat_m = nn.ModuleList([])\n        feat_m.append(self.conv1)\n        feat_m.append(self.bn1)\n        feat_m.append(self.layer1)\n        feat_m.append(self.layer2)\n        feat_m.append(self.layer3)\n        feat_m.append(self.layer4)\n        return feat_m\n\n    def get_bn_before_relu(self):\n        if isinstance(self.layer1[0], Bottleneck):\n            bn1 = self.layer1[-1].bn3\n            bn2 = self.layer2[-1].bn3\n            bn3 = self.layer3[-1].bn3\n            bn4 = self.layer4[-1].bn3\n        elif isinstance(self.layer1[0], BasicBlock):\n            bn1 = self.layer1[-1].bn2\n            bn2 = self.layer2[-1].bn2\n            bn3 = self.layer3[-1].bn2\n            bn4 = self.layer4[-1].bn2\n        else:\n            raise NotImplementedError(\"ResNet unknown block error !!!\")\n\n        return [bn1, bn2, bn3, bn4]\n\n    def get_stage_channels(self):\n        return self.stage_channels\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for i in range(num_blocks):\n            stride = strides[i]\n            layers.append(block(self.in_planes, planes, stride, i == num_blocks - 1))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def encode(self, x, idx, preact=False):\n        if idx == -1:\n            out, pre = self.layer4(F.relu(x))\n        elif idx == -2:\n            out, pre = self.layer3(F.relu(x))\n        elif idx == -3:\n            out, pre = self.layer2(F.relu(x))\n        else:\n            raise NotImplementedError()\n        return pre\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        f0 = out\n        out, f1_pre = self.layer1(out)\n        f1 = out\n        out, f2_pre = self.layer2(out)\n        f2 = out\n        out, f3_pre = self.layer3(out)\n        f3 = out\n        out, f4_pre = self.layer4(out)\n        f4 = out\n        out = self.avgpool(out)\n        avg = out.reshape(out.size(0), -1)\n        out = self.linear(avg)\n\n        feats = {}\n        feats[\"feats\"] = [f0, f1, f2, f3, f4]\n        feats[\"preact_feats\"] = [f0, f1_pre, f2_pre, f3_pre, f4_pre]\n        feats[\"pooled_feat\"] = avg\n\n        return out, feats\n\n\ndef ResNet18(**kwargs):\n    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n\n\ndef ResNet34(**kwargs):\n    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n\n\ndef ResNet50(**kwargs):\n    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n\n\ndef ResNet101(**kwargs):\n    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n\n\ndef ResNet152(**kwargs):\n    return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n\n\nif __name__ == \"__main__\":\n    net = ResNet18(num_classes=100)\n    x = torch.randn(2, 3, 32, 32)\n    logit, feats = net(x)\n\n    for f in feats[\"feats\"]:\n        print(f.shape, f.min().item())\n    print(logit.shape)\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n        super(BasicBlock, self).__init__()\n        self.is_last = is_last\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        preact = out\n        out = F.relu(out)\n        if self.is_last:\n            return out, preact\n        else:\n            return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n        super(Bottleneck, self).__init__()\n        self.is_last = is_last\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        preact = out\n        out = F.relu(out)\n        if self.is_last:\n            return out, preact\n        else:\n            return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, depth, num_filters, block_name=\"BasicBlock\", num_classes=10):\n        super(ResNet, self).__init__()\n        # Model type specifies number of layers for CIFAR-10 model\n        if block_name.lower() == \"basicblock\":\n            assert (\n                depth - 2\n            ) % 6 == 0, \"When use basicblock, depth should be 6n+2, e.g. 20, 32, 44, 56, 110, 1202\"\n            n = (depth - 2) // 6\n            block = BasicBlock\n        elif block_name.lower() == \"bottleneck\":\n            assert (\n                depth - 2\n            ) % 9 == 0, \"When use bottleneck, depth should be 9n+2, e.g. 20, 29, 47, 56, 110, 1199\"\n            n = (depth - 2) // 9\n            block = Bottleneck\n        else:\n            raise ValueError(\"block_name shoule be Basicblock or Bottleneck\")\n\n        self.inplanes = num_filters[0]\n        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_filters[0])\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, num_filters[1], n)\n        self.layer2 = self._make_layer(block, num_filters[2], n, stride=2)\n        self.layer3 = self._make_layer(block, num_filters[3], n, stride=2)\n        self.avgpool = nn.AvgPool2d(8)\n        self.fc = nn.Linear(num_filters[3] * block.expansion, num_classes)\n        self.stage_channels = num_filters\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = list([])\n        layers.append(\n            block(self.inplanes, planes, stride, downsample, is_last=(blocks == 1))\n        )\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, is_last=(i == blocks - 1)))\n\n        return nn.Sequential(*layers)\n\n    def get_feat_modules(self):\n        feat_m = nn.ModuleList([])\n        feat_m.append(self.conv1)\n        feat_m.append(self.bn1)\n        feat_m.append(self.relu)\n        feat_m.append(self.layer1)\n        feat_m.append(self.layer2)\n        feat_m.append(self.layer3)\n        return feat_m\n\n    def get_bn_before_relu(self):\n        if isinstance(self.layer1[0], Bottleneck):\n            bn1 = self.layer1[-1].bn3\n            bn2 = self.layer2[-1].bn3\n            bn3 = self.layer3[-1].bn3\n        elif isinstance(self.layer1[0], BasicBlock):\n            bn1 = self.layer1[-1].bn2\n            bn2 = self.layer2[-1].bn2\n            bn3 = self.layer3[-1].bn2\n        else:\n            raise NotImplementedError(\"ResNet unknown block error !!!\")\n\n        return [bn1, bn2, bn3]\n\n    def get_stage_channels(self):\n        return self.stage_channels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)  # 32x32\n        f0 = x\n\n        x, f1_pre = self.layer1(x)  # 32x32\n        f1 = x\n        x, f2_pre = self.layer2(x)  # 16x16\n        f2 = x\n        x, f3_pre = self.layer3(x)  # 8x8\n        f3 = x\n\n        x = self.avgpool(x)\n        avg = x.reshape(x.size(0), -1)\n        out = self.fc(avg)\n\n        feats = {}\n        feats[\"feats\"] = [f0, f1, f2, f3]\n        feats[\"preact_feats\"] = [f0, f1_pre, f2_pre, f3_pre]\n        feats[\"pooled_feat\"] = avg\n\n        return out\n\n\ndef resnet8(**kwargs):\n    return ResNet(8, [16, 16, 32, 64], \"basicblock\", **kwargs)\n\n\ndef resnet14(**kwargs):\n    return ResNet(14, [16, 16, 32, 64], \"basicblock\", **kwargs)\n\n\ndef resnet20(**kwargs):\n    return ResNet(20, [16, 16, 32, 64], \"basicblock\", **kwargs)\n\n\ndef resnet32(**kwargs):\n    return ResNet(32, [16, 16, 32, 64], \"basicblock\", **kwargs)\n\n\ndef resnet44(**kwargs):\n    return ResNet(44, [16, 16, 32, 64], \"basicblock\", **kwargs)\n\n\ndef resnet56(**kwargs):\n    return ResNet(56, [16, 16, 32, 64], \"basicblock\", **kwargs)\n\n\ndef resnet110(**kwargs):\n    return ResNet(110, [16, 16, 32, 64], \"basicblock\", **kwargs)\n\n\ndef resnet8x4(**kwargs):\n    return ResNet(8, [32, 64, 128, 256], \"basicblock\", **kwargs)\n\n\ndef resnet32x4(**kwargs):\n    return ResNet(32, [32, 64, 128, 256], \"basicblock\", **kwargs)\n\ncifar100_model_prefix = \"/kaggle/input/cifar_teachers/pytorch/default/1/cifar_teachers/\"\n\ncifar_model_dict = {\n    # teachers\n    \"resnet56\": (\n        resnet56,\n        cifar100_model_prefix + \"resnet56_vanilla/ckpt_epoch_240.pth\",\n    ),\n    \"resnet110\": (\n        resnet110,\n        cifar100_model_prefix + \"resnet110_vanilla/ckpt_epoch_240.pth\",\n    ),\n    \"resnet32x4\": (\n        resnet32x4,\n        cifar100_model_prefix + \"resnet32x4_vanilla/ckpt_epoch_240.pth\",\n    ),\n    \"ResNet50\": (\n        ResNet50,\n        cifar100_model_prefix + \"ResNet50_vanilla/ckpt_epoch_240.pth\",\n    ),\n    # \"wrn_40_2\": (\n    #     wrn_40_2,\n    #     cifar100_model_prefix + \"wrn_40_2_vanilla/ckpt_epoch_240.pth\",\n    # ),\n    # \"vgg13\": (vgg13_bn, cifar100_model_prefix + \"vgg13_vanilla/ckpt_epoch_240.pth\"),\n    # students\n    \"resnet8\": (resnet8, None),\n    \"resnet14\": (resnet14, None),\n    \"resnet20\": (resnet20, None),\n    \"resnet32\": (resnet32, None),\n    \"resnet44\": (resnet44, None),\n    \"resnet8x4\": (resnet8x4, None),\n    \"ResNet18\": (ResNet18, None),\n    # \"wrn_16_1\": (wrn_16_1, None),\n    # \"wrn_16_2\": (wrn_16_2, None),\n    # \"wrn_40_1\": (wrn_40_1, None),\n    # \"vgg8\": (vgg8_bn, None),\n    # \"vgg11\": (vgg11_bn, None),\n    # \"vgg16\": (vgg16_bn, None),\n    # \"vgg19\": (vgg19_bn, None),\n    # \"MobileNetV2\": (mobile_half, None),\n    # \"ShuffleV1\": (ShuffleV1, None),\n    # \"ShuffleV2\": (ShuffleV2, None),\n}","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:06:22.892532Z","iopub.execute_input":"2025-01-24T09:06:22.892949Z","iopub.status.idle":"2025-01-24T09:06:23.328466Z","shell.execute_reply.started":"2025-01-24T09:06:22.892927Z","shell.execute_reply":"2025-01-24T09:06:23.327636Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perception(logits, epsilon=1e-5):\n    \"\"\"\n    perform perception on logits.\n    \n    Parameters:\n    logits (torch.Tensor): A tensor of shape (B, N) where B is the batch size and N is the number of classes.\n    epsilon (float): A small constant to avoid division by zero in normalization.\n    \n    Returns:\n    torch.Tensor: perception logits.\n    \"\"\"\n    \n    batch_mean = torch.mean(logits, dim=0, keepdim=True)\n    batch_var = torch.var(logits, dim=0, keepdim=True, unbiased=False)\n    x_normalized = (logits - batch_mean) / torch.sqrt(batch_var + epsilon)\n    \n    return x_normalized\n    \n\ndef luminet_loss(logits_student, logits_teacher, target, alpha, temperature):\n    #print('Student')\n    stu_batch = perception(logits_student)\n    #print('Teacher')\n    tea_batch = perception(logits_teacher)\n    \n    pred_teacher = F.softmax(\n        tea_batch/temperature, dim=1\n    )\n    log_pred_student = F.log_softmax(\n        stu_batch/temperature,dim=1\n    )\n    nckd_loss = F.kl_div(log_pred_student, pred_teacher, reduction='batchmean')\n    nckd_loss*=alpha**2\n    \n    return nckd_loss\n\ndef perception(logits, epsilon=1e-5):\n    \"\"\"\n    perform perception on logits.\n    \n    Parameters:\n    logits (torch.Tensor): A tensor of shape (B, N) where B is the batch size and N is the number of classes.\n    epsilon (float): A small constant to avoid division by zero in normalization.\n    \n    Returns:\n    torch.Tensor: perception logits.\n    \"\"\"\n    \n    batch_mean = torch.mean(logits, dim=0, keepdim=True)\n    batch_var = torch.var(logits, dim=0, keepdim=True, unbiased=False)\n    x_normalized = (logits - batch_mean) / torch.sqrt(batch_var + epsilon)\n    \n    return x_normalized\n    \n\ndef normalize(logit):\n    mean = logit.mean(dim=-1, keepdims=True)\n    stdv = logit.std(dim=-1, keepdims=True)\n    return (logit - mean) / (1e-7 + stdv)\n\ndef kd_loss(logits_student_in, logits_teacher_in, temperature, logit_stand):\n    logits_student = normalize(logits_student_in) if logit_stand else logits_student_in\n    logits_teacher = normalize(logits_teacher_in) if logit_stand else logits_teacher_in\n    log_pred_student = F.log_softmax(logits_student / temperature, dim=1)\n    pred_teacher = F.softmax(logits_teacher / temperature, dim=1)\n    loss_kd = F.kl_div(log_pred_student, pred_teacher, reduction=\"none\").sum(1).mean()\n    loss_kd *= temperature**2\n    return loss_kd\n\nclass Distiller(nn.Module):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.student = student\n        self.teacher = teacher\n\n    def train(self, mode=True):\n        # teacher as eval mode by default\n        if not isinstance(mode, bool):\n            raise ValueError(\"training mode is expected to be boolean\")\n        self.training = mode\n        for module in self.children():\n            module.train(mode)\n        self.teacher.eval()\n        return self\n\n    def get_learnable_parameters(self):\n        # if the method introduces extra parameters, re-impl this function\n        return [v for k, v in self.student.named_parameters()]\n\n    def get_extra_parameters(self):\n        # calculate the extra parameters introduced by the distiller\n        return 0\n\n    def forward_train(self, **kwargs):\n        # training function for the distillation method\n        raise NotImplementedError()\n\n    def forward_test(self, image):\n        return self.student(image)\n\n    def forward(self, **kwargs):\n        if self.training:\n            return self.forward_train(**kwargs)\n        return self.forward_test(kwargs[\"image\"])\n\nclass DTKD(Distiller):\n    def __init__(self, student, teacher):\n        super(DTKD, self).__init__(student, teacher)\n        self.temperature = 2\n        self.ce_loss_weight = 0.1\n        self.kd_loss_weight = 9\n        self.logit_stand = True\n\n    def forward_train(self, image, target, **kwargs):\n        logits_student = self.student(image)\n        with torch.no_grad():\n            logits_teacher = self.teacher(image)\n\n        # losses\n        loss_ce = self.ce_loss_weight * F.cross_entropy(logits_student, target)\n        loss_kd = self.kd_loss_weight * kd_loss(\n            logits_student, logits_teacher, self.temperature, self.logit_stand\n        )\n        losses_dict = {\n            \"loss_ce\": loss_ce,\n            \"loss_kd\": loss_kd,\n        }\n        return logits_student, losses_dict\n        \nclass BaseTrainer(object):\n    def __init__(\n        self, \n        experiment_name, \n        distiller, \n        train_loader, \n        val_loader\n    ):\n        self.distiller = distiller\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = torch.optim.SGD(\n            self.distiller.get_learnable_parameters(), \n            lr=0.05, \n            weight_decay=5e-4,\n            momentum=0.9\n        )\n        self.best_acc = -1\n\n        username = getpass.getuser()\n        # init loggers\n        self.log_path = os.path.join(\"./output\", experiment_name)\n        if not os.path.exists(self.log_path):\n            os.makedirs(self.log_path)\n        self.tf_writer = SummaryWriter(os.path.join(self.log_path, \"train.events\"))\n\n    def adjust_learning_rate(self, epoch, optimizer):\n        steps = np.sum(epoch > np.asarray([62, 75, 87]))\n        if steps > 0:\n            new_lr = 0.05 * (0.1**steps)\n            for param_group in optimizer.param_groups:\n                param_group[\"lr\"] = new_lr\n            return new_lr\n        return 0.05\n\n    def log(self, lr, epoch, log_dict):\n        # tensorboard log\n        for k, v in log_dict.items():\n            self.tf_writer.add_scalar(k, v, epoch)\n        self.tf_writer.flush()\n\n        # wandb.init(\n        #     project=\"DTKD\",  # Replace with your project name\n        #     name=\"DTKD\",      # Optional: Give your run a name\n        #     config={                     # Optional: Add configuration details\n        #         \"learning_rate\": 0.05,\n        #         \"batch_size\": 128,\n        #         \"epochs\": 3,\n        #     }\n        # )\n        # wandb.log({\"current lr\": lr})\n        # wandb.log(log_dict)\n        if log_dict[\"test_acc\"] > self.best_acc:\n            self.best_acc = log_dict[\"test_acc\"]\n        #     wandb.run.summary[\"best_acc\"] = self.best_acc\n        # worklog.txt\n        with open(os.path.join(self.log_path, \"worklog.txt\"), \"a\") as writer:\n            lines = [\n                \"-\" * 25 + os.linesep,\n                \"epoch: {}\".format(epoch) + os.linesep,\n                \"lr: {:.2f}\".format(float(lr)) + os.linesep,\n            ]\n            for k, v in log_dict.items():\n                lines.append(\"{}: {:.2f}\".format(k, v) + os.linesep)\n            lines.append(\"-\" * 25 + os.linesep)\n            writer.writelines(lines)\n\n    def train(self, resume=False, num_epochs=100):\n        epoch = 1\n        if resume:\n            state = load_checkpoint(os.path.join(self.log_path, \"latest\"))\n            epoch = state[\"epoch\"] + 1\n            self.distiller.load_state_dict(state[\"model\"])\n            self.optimizer.load_state_dict(state[\"optimizer\"])\n            self.best_acc = state[\"best_acc\"]\n        while epoch < num_epochs + 1:\n            self.train_epoch(epoch)\n            epoch += 1\n        print(log_msg(\"Best accuracy:{}\".format(self.best_acc), \"EVAL\"))\n        with open(os.path.join(self.log_path, \"worklog.txt\"), \"a\") as writer:\n            writer.write(\"best_acc\\t\" + \"{:.2f}\".format(float(self.best_acc)))\n\n    def train_epoch(self, epoch):\n        lr = self.adjust_learning_rate(epoch, self.optimizer)\n        train_meters = {\n            \"training_time\": AverageMeter(),\n            \"data_time\": AverageMeter(),\n            \"losses\": AverageMeter(),\n            \"top1\": AverageMeter(),\n            \"top5\": AverageMeter(),\n        }\n        num_iter = len(self.train_loader)\n        pbar = tqdm(range(num_iter))\n\n        # train loops\n        self.distiller.train()\n        for idx, data in enumerate(self.train_loader):\n            msg, train_loss = self.train_iter(data, epoch, train_meters)\n            pbar.set_description(log_msg(msg, \"TRAIN\"))\n            pbar.update()\n        pbar.close()\n\n        test_acc, test_acc_top5, test_loss = validate(self.val_loader, self.distiller)\n\n        dtkd_losses.append({\"train_loss\": train_loss, \"test_loss\": test_loss})\n        dtkd_accuracies.append({\"acc@1\": test_acc.item(), \"acc@5\": test_acc_top5.item()})\n        # log\n        log_dict = OrderedDict(\n            {\n                \"train_acc\": train_meters[\"top1\"].avg,\n                \"train_loss\": train_meters[\"losses\"].avg,\n                \"test_acc\": test_acc,\n                \"test_acc_top5\": test_acc_top5,\n                \"test_loss\": test_loss,\n            }\n        )\n        self.log(lr, epoch, log_dict)\n        # saving checkpoint\n        state = {\n            \"epoch\": epoch,\n            \"model\": self.distiller.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"best_acc\": self.best_acc,\n        }\n        student_state = {\"model\": self.distiller.student.state_dict()}\n        save_checkpoint(state, os.path.join(self.log_path, \"latest\"))\n        save_checkpoint(\n            student_state, os.path.join(self.log_path, \"student_latest\")\n        )\n        if epoch % 20 == 0:\n            save_checkpoint(\n                state, os.path.join(self.log_path, \"epoch_{}\".format(epoch))\n            )\n            save_checkpoint(\n                student_state,\n                os.path.join(self.log_path, \"student_{}\".format(epoch)),\n            )\n        # update the best\n        if test_acc >= self.best_acc:\n            save_checkpoint(state, os.path.join(self.log_path, \"best\"))\n            save_checkpoint(\n                student_state, os.path.join(self.log_path, \"student_best\")\n            )\n\n    def train_iter(self, data, epoch, train_meters):\n        self.optimizer.zero_grad()\n        train_start_time = time.time()\n        image, target = data  # Adjusted to match the output of your data loader\n        train_meters[\"data_time\"].update(time.time() - train_start_time)\n        image = image.float()\n        image = image.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n    \n        # forward\n        preds, losses_dict = self.distiller(image=image, target=target, epoch=epoch)\n    \n        # backward\n        loss = sum([l.mean() for l in losses_dict.values()])\n        loss.backward()\n        self.optimizer.step()\n        train_meters[\"training_time\"].update(time.time() - train_start_time)\n        # collect info\n        batch_size = image.size(0)\n        acc1, acc5 = accuracy(preds, target, topk=(1, 5))\n        train_meters[\"losses\"].update(loss.cpu().detach().numpy().mean(), batch_size)\n        train_meters[\"top1\"].update(acc1[0], batch_size)\n        train_meters[\"top5\"].update(acc5[0], batch_size)\n        # print info\n        msg = \"Epoch:{}| Time(data):{:.3f}| Time(train):{:.3f}| Loss:{:.4f}| Top-1:{:.3f}| Top-5:{:.3f}\".format(\n            epoch,\n            train_meters[\"data_time\"].avg,\n            train_meters[\"training_time\"].avg,\n            train_meters[\"losses\"].avg,\n            train_meters[\"top1\"].avg,\n            train_meters[\"top5\"].avg,\n        )\n        return (msg, train_meters[\"losses\"].avg)\n        \nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef validate(val_loader, distiller):\n    batch_time, losses, top1, top5 = [AverageMeter() for _ in range(4)]\n    criterion = nn.CrossEntropyLoss()\n    num_iter = len(val_loader)\n    pbar = tqdm(range(num_iter))\n\n    distiller.eval()\n    with torch.no_grad():\n        start_time = time.time()\n        for idx, (image, target) in enumerate(val_loader):\n            image = image.float()\n            image = image.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n            output = distiller(image=image)\n            loss = criterion(output, target)\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            batch_size = image.size(0)\n            losses.update(loss.cpu().detach().numpy().mean(), batch_size)\n            top1.update(acc1[0], batch_size)\n            top5.update(acc5[0], batch_size)\n\n            # measure elapsed time\n            batch_time.update(time.time() - start_time)\n            start_time = time.time()\n            msg = \"Top-1:{top1.avg:.3f}| Top-5:{top5.avg:.3f}\".format(\n                top1=top1, top5=top5\n            )\n            pbar.set_description(log_msg(msg, \"EVAL\"))\n            pbar.update()\n    pbar.close()\n    return top1.avg, top5.avg, losses.avg\n\ndef log_msg(msg, mode=\"INFO\"):\n    color_map = {\n        \"INFO\": 36,\n        \"TRAIN\": 32,\n        \"EVAL\": 31,\n    }\n    msg = \"\\033[{}m[{}] {}\\033[0m\".format(color_map[mode], mode, msg)\n    return msg\n\ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\ndef save_checkpoint(obj, path):\n    with open(path, \"wb\") as f:\n        torch.save(obj, f)\n\ndef load_checkpoint(path):\n    with open(path, \"rb\") as f:\n        return torch.load(f, map_location=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2025-01-24T10:09:23.513962Z","iopub.execute_input":"2025-01-24T10:09:23.514299Z","iopub.status.idle":"2025-01-24T10:09:23.548377Z","shell.execute_reply.started":"2025-01-24T10:09:23.514275Z","shell.execute_reply":"2025-01-24T10:09:23.547256Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LossManager:\n    def __init__(\n        self, \n        alpha, \n        beta, \n        gamma, \n        initial_temperature,\n        min_temperature\n    ):\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.current_temperature = initial_temperature\n        self.min_temperature = min_temperature\n\n    def normalize(self, logit):\n        mean = logit.mean(dim=-1, keepdims=True)\n        stdv = logit.std(dim=-1, keepdims=True)\n        \n        return (logit - mean) / (1e-7 + stdv)\n    \n    def kd_loss(self, logits_student_in, logits_teacher_in, logit_stand=True):\n        temperature = self.current_temperature\n        \n        logits_student = self.normalize(logits_student_in) \n        logits_teacher = self.normalize(logits_teacher_in)\n        log_pred_student = F.log_softmax(logits_student / temperature, dim=1)\n        \n        pred_teacher = F.softmax(logits_teacher / temperature, dim=1)\n        loss_kd = F.kl_div(log_pred_student, pred_teacher, reduction=\"none\").sum(1).mean()\n        loss_kd *= temperature*temperature\n        \n        return loss_kd\n\n    def perception(self, logits, epsilon=1e-5):\n        \"\"\"\n        perform perception on logits.\n        \n        Parameters:\n        logits (torch.Tensor): A tensor of shape (B, N) where B is the batch size and N is the number of classes.\n        epsilon (float): A small constant to avoid division by zero in normalization.\n        \n        Returns:\n        torch.Tensor: perception logits.\n        \"\"\"\n        batch_mean = torch.mean(logits, dim=0, keepdim=True)\n        batch_var = torch.var(logits, dim=0, keepdim=True, unbiased=False)\n        x_normalized = (logits - batch_mean) / torch.sqrt(batch_var + epsilon)\n    \n        return x_normalized\n    \n    def luminet_loss(self, logits_student, logits_teacher, target):\n        temperature = self.current_temperature\n        stu_batch = self.perception(logits_student)\n        tea_batch = self.perception(logits_teacher)\n    \n        pred_teacher = F.softmax(tea_batch/temperature, dim=1)\n        log_pred_student = F.log_softmax(stu_batch/temperature,dim=1)\n        \n        nckd_loss = F.kl_div(log_pred_student, pred_teacher, reduction='batchmean')\n        nckd_loss *= (33.0*33.0)\n\n        return nckd_loss\n        \n    def cosine_loss(self, student_logits, teacher_logits):\n        \"\"\"\n        Compute cosine similarity loss between student and teacher logits.\n\n        Args:\n            student_logits (torch.Tensor): Logits from student model.\n            teacher_logits (torch.Tensor): Logits from teacher model.\n\n        Returns:\n            torch.Tensor: Cosine similarity loss.\n        \"\"\"\n        # Normalize logits\n        student_norm = F.normalize(student_logits, p=2, dim=1)\n        teacher_norm = F.normalize(teacher_logits, p=2, dim=1)\n        \n        # Compute cosine similarity loss\n        cosine_loss = 1 - F.cosine_similarity(student_norm, teacher_norm).mean()\n        return cosine_loss\n\n    def rmse_loss(self, student_logits, teacher_logits):\n        \"\"\"\n        Compute Root Mean Square Error (RMSE) between student and teacher logits.\n\n        Args:\n            student_logits (torch.Tensor): Logits from student model.\n            teacher_logits (torch.Tensor): Logits from teacher model.\n\n        Returns:\n            torch.Tensor: RMSE loss.\n        \"\"\"\n        \n        rmse = torch.sqrt(F.mse_loss(student_logits, teacher_logits))\n        return rmse\n        \n    def mae_loss(self, student_logits, teacher_logits):\n        \"\"\"\n        Compute Root Mean Square Error (RMSE) between student and teacher logits.\n\n        Args:\n            student_logits (torch.Tensor): Logits from student model.\n            teacher_logits (torch.Tensor): Logits from teacher model.\n\n        Returns:\n            torch.Tensor: RMSE loss.\n        \"\"\"\n        \n        rmse = torch.nn.L1Loss()(student_logits, teacher_logits)\n        return rmse\n\n    def hard_loss(self, student_logits, outputs):\n        \"\"\"\n        Compute hard loss (cross-entropy) between student logits and true labels.\n\n        Args:\n            student_logits (torch.Tensor): Logits from student model.\n            outputs (torch.Tensor): True labels.\n\n        Returns:\n            torch.Tensor: Cross-entropy loss.\n        \"\"\"\n        \n        return torch.nn.CrossEntropyLoss()(student_logits, outputs)\n\n    def soft_distillation_loss(self, student_logits, teacher_logits):\n        \"\"\"\n        Compute knowledge distillation loss with dynamic temperature.\n\n        Args:\n            student_logits (torch.Tensor): Logits from student model.\n            teacher_logits (torch.Tensor): Logits from teacher model.\n\n        Returns:\n            torch.Tensor: Knowledge distillation loss.\n        \"\"\"\n        soft_targets = F.softmax(teacher_logits / self.current_temperature, dim=1)\n        soft_predictions = F.log_softmax(student_logits / self.current_temperature, dim=1)\n        \n        loss = F.kl_div(soft_predictions, soft_targets, reduction='batchmean')\n        return loss * (self.current_temperature ** 2)\n\n    def combined_loss(self, student_logits, teacher_logits, outputs):\n        \"\"\"Only include the additional losses (cosine and RMSE) here\"\"\"\n        # Cosine loss\n        cosine_loss = self.beta * self.cosine_loss(student_logits, teacher_logits)\n        # RMSE loss\n        rmse_loss = self.gamma * self.rmse_loss(student_logits, teacher_logits)\n        return cosine_loss + rmse_loss\n    \nclass DynamicTemperatureScheduler(nn.Module):\n    \"\"\"\n    Dynamic Temperature Scheduler for Knowledge Distillation.\n\n    Args:\n        initial_temperature (float): Starting temperature value.\n        min_temperature (float): Minimum allowable temperature.\n        max_temperature (float): Maximum allowable temperature.\n        schedule_type (str): Type of temperature scheduling strategy.\n        loss_type (str): Type of loss to use (combined or general KD).\n        alpha (float): Importance for soft loss, 1-alpha for hard loss.\n        beta (float): Importance of cosine loss.\n        gamma (float): Importance for RMSE loss.\n    \"\"\"\n    def __init__(\n        self, \n        initial_temperature=8.0, \n        min_temperature=4.0, \n        max_temperature=8,\n        max_epoch=50,\n        warmup=20,\n        alpha=0.5,\n        beta=0.9,\n        gamma=0.5,\n    ):\n        super(DynamicTemperatureScheduler, self).__init__()\n\n        self.current_temperature = initial_temperature\n        self.initial_temperature = initial_temperature\n        self.min_temperature = min_temperature\n        self.max_temperature = max_temperature\n        self.max_epoch = max_epoch\n        self.warmup = warmup\n        \n        # Tracking training dynamics\n        self.loss_history = []\n        self.student_loss = []\n\n        # Constants for importance\n        self.loss_manager = LossManager(\n            alpha, \n            beta, \n            gamma, \n            initial_temperature,\n            min_temperature\n        )\n        \n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        \n    def update_temperature(self, current_epoch, loss_divergence):\n        progress = torch.tensor(current_epoch / self.max_epoch)\n        cosine_factor = 0.5 * (1 + torch.cos(torch.pi * progress))\n        log_loss = torch.log(1 + torch.tensor(loss_divergence))\n        adaptive_scale = log_loss/ (log_loss + 1)\n\n        if adaptive_scale > 1:\n            target_temperature = self.initial_temperature * cosine_factor * (1 + adaptive_scale)\n        else:\n            target_temperature = self.initial_temperature * cosine_factor\n        \n        target_temperature = torch.clamp(\n            target_temperature, \n            self.min_temperature, \n            self.max_temperature\n        )\n        \n        momentum = 0.9\n        self.current_temperature = momentum * self.current_temperature + (1 - momentum) * target_temperature\n\n        self.loss_manager.current_temperature = self.current_temperature\n        \n    def get_temperature(self):\n        \"\"\"\n        Retrieve current temperature value.\n\n        Returns:\n            float: Current dynamic temperature.\n        \"\"\"\n        \n        return self.current_temperature\n        \n    def forward(self, epoch, student_logits, teacher_logits, outputs, loss_type=\"kd++\"):\n        \"\"\"\n        Forward pass to compute the loss based on the specified loss type.\n\n        Args:\n            student_logits (torch.Tensor): Logits from student model.\n            teacher_logits (torch.Tensor): Logits from teacher model.\n            outputs (torch.Tensor): True labels.\n\n        Returns:\n            torch.Tensor: Computed loss.\n        \"\"\"\n        if loss_type == \"ours\":\n            temp_ratio = (self.current_temperature - 1.0) / (3.0 - 1.0)\n            temp_ratio = max(0, min(1, temp_ratio))\n            \n            # Base losses (always present)\n            soft_loss = self.loss_manager.soft_distillation_loss(\n                student_logits, \n                teacher_logits\n            )\n            \n            hard_loss = self.loss_manager.hard_loss(\n                student_logits, \n                outputs\n            )\n            \n            teacher_loss = self.loss_manager.hard_loss(\n                teacher_logits, \n                outputs\n            )\n            \n            # Temperature-dependent weighting for soft vs hard\n            if self.current_temperature > 1:\n                soft_weight = self.alpha * temp_ratio + 0.4 * (1 - temp_ratio)\n                hard_weight = (1 - self.alpha) * temp_ratio + 0.5 * (1 - temp_ratio)\n            else:\n                soft_weight = 0.2\n                hard_weight = 0.5\n                \n            # Additional losses only when temperature is higher\n            additional_losses = temp_ratio * self.loss_manager.combined_loss(\n                student_logits, \n                teacher_logits, \n                outputs\n            )\n                \n            warmup = 1 if self.warmup == None else min(epoch / self.warmup, 1.0)\n            \n            total_loss = (\n                soft_weight * soft_loss + \n                hard_weight * hard_loss + \n                additional_losses\n            )\n            \n            return  warmup * total_loss\n            \n        elif loss_type == \"luminet\":\n            warmup = 1 if self.warmup == None else min(epoch / self.warmup, 1.0)\n            \n            loss_ce = (2.0) * F.cross_entropy(\n                student_logits, \n                outputs\n            )\n            \n            loss_luminet = warmup * self.loss_manager.luminet_loss(\n                student_logits,\n                teacher_logits,\n                outputs\n            )\n\n            losses_dict = {\n                \"loss_ce\": loss_ce,\n                \"loss_kd\": loss_luminet,\n            }\n\n            return sum([l.mean() for l in losses_dict.values()])\n\n        elif loss_type == \"kd++\":\n            logits_student = student_logits\n            logits_teacher = teacher_logits\n    \n            target = outputs\n            \n            loss_ce = 0.1 * F.cross_entropy(logits_student, target)\n            \n            loss_kd = 9 * self.loss_manager.kd_loss(\n                logits_student, logits_teacher\n            )\n            \n            losses_dict = {\n                \"loss_ce\": loss_ce,\n                \"loss_kd\": loss_kd,\n            }\n\n            return sum([l.mean() for l in losses_dict.values()])\n","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:06:23.363787Z","iopub.execute_input":"2025-01-24T09:06:23.364033Z","iopub.status.idle":"2025-01-24T09:06:23.385809Z","shell.execute_reply.started":"2025-01-24T09:06:23.364001Z","shell.execute_reply":"2025-01-24T09:06:23.385120Z"},"papermill":{"duration":0.023016,"end_time":"2024-12-28T15:03:24.769399","exception":false,"start_time":"2024-12-28T15:03:24.746383","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport os\nimport numpy as np\n\ndef calculate_accuracy(outputs, targets, topk=(1, 5)):\n    \"\"\"\n    Calculate top-k accuracy\n    \n    Args:\n        outputs (torch.Tensor): Model predictions\n        targets (torch.Tensor): Ground truth labels\n        topk (tuple): Top-k values to compute accuracy\n    \n    Returns:\n        list: Top-k accuracies\n    \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = targets.size(0)\n\n        # Get top-k predictions\n        _, pred = outputs.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n\n        # Calculate accuracies\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        \n        return res\n\ndef adjust_learning_rate(epoch, lr, optimizer):\n    steps = np.sum(epoch > np.asarray([62, 75, 87]))\n    if steps > 0:\n        new_lr = 0.05 * (0.1**steps)\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = new_lr\n        return new_lr\n    return lr\n\ndef train_knowledge_distillation(\n    name,\n    teacher_model, \n    student_model, \n    train_loader, \n    val_loader,\n    optimizer,\n    lr,\n    epochs=50, \n    val_steps=10,\n    temperature_scheduler=None,\n    scheduler=None,\n    save_path=\"./output/\"\n):\n    \"\"\"\n    Train student model with periodic validation\n    \n    Args:\n        teacher_model (nn.Module): Pre-trained teacher model\n        student_model (nn.Module): Model to be distilled\n        train_dataset (Dataset): Training data\n        val_dataset (Dataset): Validation data\n        epochs (int): Total training epochs\n        alpha (float): Loss balancing coefficient\n        temperature_scheduler (DynamicTemperatureScheduler): Temperature scheduler\n        save_path (str): Path to save the best model\n    \"\"\"\n    \n    run = wandb.init(\n        # Set the project where this run will be logged\n        project=\"DTAD_Trials\",\n        name=name\n    )\n    \n    # Optimizer and criterion\n    student_optimizer = optimizer\n    task_criterion = torch.nn.CrossEntropyLoss()\n    \n    # Set models to appropriate modes\n    teacher_model.eval()\n    val_loss = 0\n    top1_acc = 0\n    top5_acc = 0\n    \n    print(\"-\" * 15 + \" Teacher Validation \" + \"-\" * 15)\n    with torch.no_grad():\n        for val_x, val_y in val_loader:\n            val_x, val_y = val_x.to(\"cuda\"), val_y.to(\"cuda\")\n            val_outputs = teacher_model(val_x)\n            val_batch_loss = task_criterion(val_outputs, val_y)\n            val_loss += val_batch_loss.item()\n            \n            # Calculate accuracies\n            batch_top1, batch_top5 = calculate_accuracy(val_outputs, val_y)\n            top1_acc += batch_top1.item()\n            top5_acc += batch_top5.item()\n    \n    # Average validation metrics\n    val_loss /= len(val_loader)\n    top1_acc /= len(val_loader)\n    top5_acc /= len(val_loader)\n    \n    print(f\"Val Loss: {val_loss:.4f} | \"\n          f\"Top-1 Accuracy: {top1_acc:.2f}% | Top-5 Accuracy: {top5_acc:.2f}%\")\n    print(\"-\" * 50)\n\n    best_top1_acc = 0.0  # Initialize best accuracy tracker\n    \n    # Training loop\n    for epoch in range(epochs):\n        # Training phase\n        print(\"-\" * 16 + \" Training \" + \"-\" * 16)\n        student_model.train()\n        train_loss = 0\n        train_acc_1 = 0\n        train_acc_5 = 0\n\n        lr = adjust_learning_rate(epoch+1, lr, student_optimizer) if scheduler == None else 0\n\n        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n            batch_x, batch_y = batch_x.to('cuda'), batch_y.to('cuda')\n            \n            # Forward passes\n            with torch.no_grad():\n                teacher_logits = teacher_model(batch_x)\n                teacher_loss = task_criterion(teacher_logits, batch_y)\n\n            student_logits = student_model(batch_x)\n            student_loss = task_criterion(student_logits, batch_y)\n            \n            # Knowledge distillation loss\n            if temperature_scheduler:\n                # Combine losses\n                total_batch_loss = temperature_scheduler(\n                    epoch,\n                    student_logits=student_logits,\n                    teacher_logits=teacher_logits,\n                    outputs=batch_y\n                )\n\n                temperature_scheduler.update_temperature(\n                    current_epoch=epoch, \n                    loss_divergence=teacher_loss.item()-student_loss.item()\n                )\n\n                # Backward pass and optimization\n                student_optimizer.zero_grad()\n                total_batch_loss.backward()\n                student_optimizer.step()\n            \n            # Calculate accuracies\n            acc1, acc5 = calculate_accuracy(student_logits, batch_y) \n            train_loss += total_batch_loss.item()\n            train_acc_1 += acc1.item()\n            train_acc_5 += acc5.item()\n\n            if batch_idx % 100 == 0:\n                print(f\"Epoch {epoch+1} | Batch {batch_idx}/{len(train_loader)} | \"\n                      f\"Loss: {total_batch_loss.item():.4f} | Temp: {temperature_scheduler.get_temperature():.2f} | \"\n                      f\"Acc@1: {acc1.item():.2f}% | Acc@5: {acc5.item():.2f}%\")\n        \n        # Epoch-end metrics\n        train_loss /= len(train_loader)\n        train_acc_1 /= len(train_loader)\n        train_acc_5 /= len(train_loader)\n        \n        if scheduler != None:\n            scheduler.step()\n            lr = scheduler.get_last_lr()[0]\n        \n        print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {train_loss:.4f} | \"\n              f\"Acc@1: {train_acc_1:.2f}% | Acc@5: {train_acc_5:.2f}%\")\n        print(\"-\" * 42)\n\n    # if (epoch+1) % val_steps == 0:    \n        # Validation phase\n        print(\"-\" * 15 + \" Validation \" + \"-\" * 15)\n        student_model.eval()\n        val_loss = 0\n        top1_acc = 0\n        top5_acc = 0\n        \n        with torch.no_grad():\n            for val_x, val_y in val_loader:\n                val_x, val_y = val_x.to(\"cuda\"), val_y.to(\"cuda\")\n                val_outputs = student_model(val_x)\n                val_batch_loss = task_criterion(val_outputs, val_y)\n                val_loss += val_batch_loss.item()\n                \n                # Calculate accuracies\n                batch_top1, batch_top5 = calculate_accuracy(val_outputs, val_y)\n                top1_acc += batch_top1.item()\n                top5_acc += batch_top5.item()\n        \n        # Average validation metrics\n        val_loss /= len(val_loader)\n        top1_acc /= len(val_loader)\n        top5_acc /= len(val_loader)\n\n        wandb.log(\n            {\n                \"train_acc\": train_acc_1, \n                \"train_loss\": train_loss,\n                \"val_acc\": top1_acc, \n                \"val_loss\": val_loss,\n                \"lr\": lr,\n                \"temp\": temperature_scheduler.get_temperature()\n            }\n        )\n\n        our_losses.append({\"train_loss\": train_loss, \"test_loss\": val_loss})\n        our_accuracies.append({\"acc@1\": top1_acc, \"acc@5\": top5_acc})\n        \n        print(f\"Epoch {epoch+1}/{epochs} | Val Loss: {val_loss:.4f} | \"\n              f\"Top-1 Accuracy: {top1_acc:.2f}% | Top-5 Accuracy: {top5_acc:.2f}%\")\n        print(\"-\" * 42)\n        \n        # Save the best model\n        if top1_acc > best_top1_acc:\n            best_top1_acc = top1_acc\n            torch.save(student_model.state_dict(), f\"DTAD_@{top1_acc}.pth\")\n            print(f\"Best model saved at epoch {epoch+1} with Top-1 Accuracy: {best_top1_acc:.2f}%\")\n    print(\"Best Model Accuracy: \", best_top1_acc)\n    run.finish()\n    \n    torch.save(student_model.state_dict(), \"trained_studentDTAD.pth\")\n    return student_model","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:06:23.386533Z","iopub.execute_input":"2025-01-24T09:06:23.386772Z","iopub.status.idle":"2025-01-24T09:06:23.405548Z","shell.execute_reply.started":"2025-01-24T09:06:23.386741Z","shell.execute_reply":"2025-01-24T09:06:23.404681Z"},"papermill":{"duration":0.020375,"end_time":"2024-12-28T15:03:24.792018","exception":false,"start_time":"2024-12-28T15:03:24.771643","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# CIFAR-10 Data Preparation\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # Mean and std of CIFAR-10\n])\n\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n])\n\n# Load CIFAR-10 dataset\ntrain_dataset = torchvision.datasets.CIFAR100(root=\"./data\", train=True, transform=transform, download=True)\nval_dataset = torchvision.datasets.CIFAR100(root=\"./data\", train=False, transform=val_transform, download=True)\nnum_classes = len(train_dataset.classes)\n\nbatch_size=128\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=256)\n\nteacher_model, path = cifar_model_dict[\"resnet56\"]\nteacher_model = teacher_model(num_classes=num_classes)\nteacher_model.load_state_dict(torch.load(path)[\"model\"])\nteacher_model.to(\"cuda\")\n\nprint(\"model_loaded\")","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:06:23.406300Z","iopub.execute_input":"2025-01-24T09:06:23.406608Z","iopub.status.idle":"2025-01-24T09:06:32.332619Z","shell.execute_reply.started":"2025-01-24T09:06:23.406578Z","shell.execute_reply":"2025-01-24T09:06:32.331858Z"},"papermill":{"duration":7219.361104,"end_time":"2024-12-28T17:03:44.155369","exception":false,"start_time":"2024-12-28T15:03:24.794265","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"student_model, path = cifar_model_dict[\"resnet20\"]\nstudent_model = student_model(num_classes=num_classes)\nstudent_model.to(\"cuda\")\nprint(\"models loaded\")\n\nmax_epoch = 100\nlr = 0.05\n\noptimizer = torch.optim.SGD(\n    student_model.parameters(), \n    lr=lr,\n    weight_decay=5e-4,\n    momentum=0.9\n)\n\ntemp_scheduler = DynamicTemperatureScheduler(\n    initial_temperature=4.0, \n    min_temperature=2.0, \n    max_temperature=8,\n    max_epoch=max_epoch, \n    warmup=20\n)\n\ntrained_student = train_knowledge_distillation(\n    \"56->20 (Ours) + kd+norm4->2 + log scaling\",\n    teacher_model, \n    student_model, \n    train_loader, \n    val_loader,\n    optimizer=optimizer,\n    lr=lr,\n    epochs=max_epoch,\n    val_steps=1,\n    temperature_scheduler=temp_scheduler,\n)","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:06:32.334395Z","iopub.execute_input":"2025-01-24T09:06:32.334604Z","iopub.status.idle":"2025-01-24T10:01:29.968438Z","shell.execute_reply.started":"2025-01-24T09:06:32.334585Z","shell.execute_reply":"2025-01-24T10:01:29.967573Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"student_model, path = cifar_model_dict[\"resnet20\"]\nstudent_model = student_model(num_classes=num_classes)\nstudent_model.to(\"cuda\")\n\ndistiller = DTKD(student_model, teacher_model)\n\n# # Initialize the CRDTrainer\ntrainer = BaseTrainer(\n    experiment_name=\"DTKD\",\n    distiller=distiller,\n    train_loader=train_loader, \n    val_loader=val_loader\n)\n\ntrainer.train(num_epochs=max_epoch)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-01-24T10:09:57.568644Z","iopub.execute_input":"2025-01-24T10:09:57.568956Z","execution_failed":"2025-01-24T11:03:41.407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_losses():\n    # Extracting train and test losses for plotting\n    dtkd_train_loss = [entry['train_loss'] for entry in dtkd_losses]\n    dtkd_test_loss = [entry['test_loss'] for entry in dtkd_losses]\n    our_train_loss = [entry['train_loss'] for entry in our_losses]\n    our_test_loss = [entry['test_loss'] for entry in our_losses]\n    \n    # FOR 100 EPOCH\n    # Plotting\n    plt.figure(figsize=(8, 6)) \n    \n    # Train Losses\n    plt.subplot(2, 1, 1)  # Positioning in the first row\n    plt.plot(dtkd_train_loss, label=\"DTKD Train Loss\", color='blue')\n    plt.plot(our_train_loss, label=\"Our Train Loss\", color='red')\n    plt.title(\"Train Losses\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid()\n    \n    # Test Losses\n    plt.subplot(2, 1, 2)  # Positioning in the second row\n    plt.plot(dtkd_test_loss, label=\"DTKD Test Loss\", color='blue')\n    plt.plot(our_test_loss, label=\"Our Test Loss\", color='red')\n    plt.title(\"Test Losses\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid()\n    \n    plt.tight_layout()  # Adjust layout to avoid overlap\n    plt.show()\n\nplot_losses()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T10:01:29.973763Z","iopub.execute_input":"2025-01-24T10:01:29.973947Z","iopub.status.idle":"2025-01-24T10:01:30.496111Z","shell.execute_reply.started":"2025-01-24T10:01:29.973931Z","shell.execute_reply":"2025-01-24T10:01:30.495193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_accuracies():\n    \n    # Extract data\n    dtkd_acc1 = [entry['acc@1'] for entry in dtkd_accuracies]\n    our_acc1 = [entry['acc@1'] for entry in our_accuracies]\n    \n    # Plotting\n    plt.figure(figsize=(8, 6))\n    plt.plot(dtkd_acc1, label=\"DTKD acc@1\", color='blue')\n    plt.plot(our_acc1, label=\"Our acc@1\", color='red')\n    \n    # Graph details\n    plt.title(\"Accuracy Comparison\", fontsize=16)\n    plt.xlabel(\"Epoch\", fontsize=12)\n    plt.ylabel(\"Accuracy (%)\", fontsize=12)\n    plt.legend()\n    plt.grid()\n    plt.tight_layout()\n    \n    # Show plot\n    plt.show()\n    \nplot_accuracies()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T10:01:30.496868Z","iopub.execute_input":"2025-01-24T10:01:30.497072Z","iopub.status.idle":"2025-01-24T10:01:30.818339Z","shell.execute_reply.started":"2025-01-24T10:01:30.497054Z","shell.execute_reply":"2025-01-24T10:01:30.817494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}