{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 219664,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 187338,
     "modelId": 209416
    }
   ],
   "dockerImageVersionId": 30823,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "resnet101-34-kd",
   "provenance": []
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7227.877548,
   "end_time": "2024-12-28T17:03:45.676396",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-28T15:03:17.798848",
   "version": "2.6.0"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install tensorboard tensorboardX -q\nimport math\nimport os\nimport time\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport getpass\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nimport sys\n\nfrom __future__ import absolute_import\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom torchvision.models import ResNet\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet18, resnet101, resnet34\nfrom torch.utils.data import DataLoader\nimport math\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport wandb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-25T15:27:30.337676Z",
     "iopub.execute_input": "2025-01-25T15:27:30.337883Z",
     "iopub.status.idle": "2025-01-25T15:27:46.557562Z",
     "shell.execute_reply.started": "2025-01-25T15:27:30.337856Z",
     "shell.execute_reply": "2025-01-25T15:27:46.556763Z"
    },
    "papermill": {
     "duration": 4.486292,
     "end_time": "2024-12-28T15:03:24.743169",
     "exception": false,
     "start_time": "2024-12-28T15:03:20.256877",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-25T15:27:46.558414Z",
     "iopub.execute_input": "2025-01-25T15:27:46.558656Z",
     "iopub.status.idle": "2025-01-25T15:27:53.699706Z",
     "shell.execute_reply.started": "2025-01-25T15:27:46.558633Z",
     "shell.execute_reply": "2025-01-25T15:27:53.698994Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-25T15:27:53.700912Z",
     "iopub.execute_input": "2025-01-25T15:27:53.701391Z",
     "iopub.status.idle": "2025-01-25T15:27:54.092792Z",
     "shell.execute_reply.started": "2025-01-25T15:27:53.701368Z",
     "shell.execute_reply": "2025-01-25T15:27:54.092004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def perception(logits, epsilon=1e-5):\n    \"\"\"\n    perform perception on logits.\n    \n    Parameters:\n    logits (torch.Tensor): A tensor of shape (B, N) where B is the batch size and N is the number of classes.\n    epsilon (float): A small constant to avoid division by zero in normalization.\n    \n    Returns:\n    torch.Tensor: perception logits.\n    \"\"\"\n    \n    batch_mean = torch.mean(logits, dim=0, keepdim=True)\n    batch_var = torch.var(logits, dim=0, keepdim=True, unbiased=False)\n    x_normalized = (logits - batch_mean) / torch.sqrt(batch_var + epsilon)\n    \n    return x_normalized\n    \n\ndef luminet_loss(logits_student, logits_teacher, target, alpha, temperature):\n    #print('Student')\n    stu_batch = perception(logits_student)\n    #print('Teacher')\n    tea_batch = perception(logits_teacher)\n    \n    pred_teacher = F.softmax(\n        tea_batch/temperature, dim=1\n    )\n    log_pred_student = F.log_softmax(\n        stu_batch/temperature,dim=1\n    )\n    nckd_loss = F.kl_div(log_pred_student, pred_teacher, reduction='batchmean')\n    nckd_loss*=alpha**2\n    \n    return nckd_loss\n\ndef perception(logits, epsilon=1e-5):\n    \"\"\"\n    perform perception on logits.\n    \n    Parameters:\n    logits (torch.Tensor): A tensor of shape (B, N) where B is the batch size and N is the number of classes.\n    epsilon (float): A small constant to avoid division by zero in normalization.\n    \n    Returns:\n    torch.Tensor: perception logits.\n    \"\"\"\n    \n    batch_mean = torch.mean(logits, dim=0, keepdim=True)\n    batch_var = torch.var(logits, dim=0, keepdim=True, unbiased=False)\n    x_normalized = (logits - batch_mean) / torch.sqrt(batch_var + epsilon)\n    \n    return x_normalized\n    \n\ndef normalize(logit):\n    mean = logit.mean(dim=-1, keepdims=True)\n    stdv = logit.std(dim=-1, keepdims=True)\n    return (logit - mean) / (1e-7 + stdv)\n\ndef kd_loss(logits_student_in, logits_teacher_in, temperature, logit_stand):\n    logits_student = normalize(logits_student_in) if logit_stand else logits_student_in\n    logits_teacher = normalize(logits_teacher_in) if logit_stand else logits_teacher_in\n    log_pred_student = F.log_softmax(logits_student / temperature, dim=1)\n    pred_teacher = F.softmax(logits_teacher / temperature, dim=1)\n    loss_kd = F.kl_div(log_pred_student, pred_teacher, reduction=\"none\").sum(1).mean()\n    loss_kd *= temperature**2\n    return loss_kd\n\nclass Distiller(nn.Module):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.student = student\n        self.teacher = teacher\n\n    def train(self, mode=True):\n        # teacher as eval mode by default\n        if not isinstance(mode, bool):\n            raise ValueError(\"training mode is expected to be boolean\")\n        self.training = mode\n        for module in self.children():\n            module.train(mode)\n        self.teacher.eval()\n        return self\n\n    def get_learnable_parameters(self):\n        # if the method introduces extra parameters, re-impl this function\n        return [v for k, v in self.student.named_parameters()]\n\n    def get_extra_parameters(self):\n        # calculate the extra parameters introduced by the distiller\n        return 0\n\n    def forward_train(self, **kwargs):\n        # training function for the distillation method\n        raise NotImplementedError()\n\n    def forward_test(self, image):\n        return self.student(image)\n\n    def forward(self, **kwargs):\n        if self.training:\n            return self.forward_train(**kwargs)\n        return self.forward_test(kwargs[\"image\"])\n\nclass DTKD(Distiller):\n    def __init__(self, student, teacher):\n        super(DTKD, self).__init__(student, teacher)\n        self.temperature = 2\n        self.ce_loss_weight = 0.1\n        self.kd_loss_weight = 9\n        self.logit_stand = True\n\n    def forward_train(self, image, target, **kwargs):\n        logits_student = self.student(image)\n        with torch.no_grad():\n            logits_teacher = self.teacher(image)\n\n        # losses\n        loss_ce = self.ce_loss_weight * F.cross_entropy(logits_student, target)\n        loss_kd = self.kd_loss_weight * kd_loss(\n            logits_student, logits_teacher, self.temperature, self.logit_stand\n        )\n        losses_dict = {\n            \"loss_ce\": loss_ce,\n            \"loss_kd\": loss_kd,\n        }\n        return logits_student, losses_dict\n        \nclass BaseTrainer(object):\n    def __init__(\n        self, \n        experiment_name, \n        distiller, \n        train_loader, \n        val_loader\n    ):\n        self.distiller = distiller\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = torch.optim.SGD(\n            self.distiller.get_learnable_parameters(), \n            lr=0.05, \n            weight_decay=5e-4,\n            momentum=0.9\n        )\n        self.best_acc = -1\n\n        username = getpass.getuser()\n        # init loggers\n        self.log_path = os.path.join(\"./output\", experiment_name)\n        if not os.path.exists(self.log_path):\n            os.makedirs(self.log_path)\n        self.tf_writer = SummaryWriter(os.path.join(self.log_path, \"train.events\"))\n\n    def adjust_learning_rate(self, epoch, optimizer):\n        steps = np.sum(epoch > np.asarray([62, 75, 87]))\n        if steps > 0:\n            new_lr = 0.05 * (0.1**steps)\n            for param_group in optimizer.param_groups:\n                param_group[\"lr\"] = new_lr\n            return new_lr\n        return 0.05\n\n    def log(self, lr, epoch, log_dict):\n        # tensorboard log\n        for k, v in log_dict.items():\n            self.tf_writer.add_scalar(k, v, epoch)\n        self.tf_writer.flush()\n\n        # wandb.init(\n        #     project=\"DTKD\",  # Replace with your project name\n        #     name=\"DTKD\",      # Optional: Give your run a name\n        #     config={                     # Optional: Add configuration details\n        #         \"learning_rate\": 0.05,\n        #         \"batch_size\": 128,\n        #         \"epochs\": 3,\n        #     }\n        # )\n        # wandb.log({\"current lr\": lr})\n        # wandb.log(log_dict)\n        if log_dict[\"test_acc\"] > self.best_acc:\n            self.best_acc = log_dict[\"test_acc\"]\n        #     wandb.run.summary[\"best_acc\"] = self.best_acc\n        # worklog.txt\n        with open(os.path.join(self.log_path, \"worklog.txt\"), \"a\") as writer:\n            lines = [\n                \"-\" * 25 + os.linesep,\n                \"epoch: {}\".format(epoch) + os.linesep,\n                \"lr: {:.2f}\".format(float(lr)) + os.linesep,\n            ]\n            for k, v in log_dict.items():\n                lines.append(\"{}: {:.2f}\".format(k, v) + os.linesep)\n            lines.append(\"-\" * 25 + os.linesep)\n            writer.writelines(lines)\n\n    def train(self, resume=False, num_epochs=100):\n        epoch = 1\n        if resume:\n            state = load_checkpoint(os.path.join(self.log_path, \"latest\"))\n            epoch = state[\"epoch\"] + 1\n            self.distiller.load_state_dict(state[\"model\"])\n            self.optimizer.load_state_dict(state[\"optimizer\"])\n            self.best_acc = state[\"best_acc\"]\n        while epoch < num_epochs + 1:\n            self.train_epoch(epoch)\n            epoch += 1\n        print(log_msg(\"Best accuracy:{}\".format(self.best_acc), \"EVAL\"))\n        with open(os.path.join(self.log_path, \"worklog.txt\"), \"a\") as writer:\n            writer.write(\"best_acc\\t\" + \"{:.2f}\".format(float(self.best_acc)))\n\n    def train_epoch(self, epoch):\n        lr = self.adjust_learning_rate(epoch, self.optimizer)\n        train_meters = {\n            \"training_time\": AverageMeter(),\n            \"data_time\": AverageMeter(),\n            \"losses\": AverageMeter(),\n            \"top1\": AverageMeter(),\n            \"top5\": AverageMeter(),\n        }\n        num_iter = len(self.train_loader)\n        pbar = tqdm(range(num_iter))\n\n        # train loops\n        self.distiller.train()\n        for idx, data in enumerate(self.train_loader):\n            msg, train_loss = self.train_iter(data, epoch, train_meters)\n            pbar.set_description(log_msg(msg, \"TRAIN\"))\n            pbar.update()\n        pbar.close()\n\n        test_acc, test_acc_top5, test_loss = validate(self.val_loader, self.distiller)\n\n        dtkd_losses.append({\"train_loss\": train_loss, \"test_loss\": test_loss})\n        dtkd_accuracies.append({\"acc@1\": test_acc.item(), \"acc@5\": test_acc_top5.item()})\n        # log\n        log_dict = OrderedDict(\n            {\n                \"train_acc\": train_meters[\"top1\"].avg,\n                \"train_loss\": train_meters[\"losses\"].avg,\n                \"test_acc\": test_acc,\n                \"test_acc_top5\": test_acc_top5,\n                \"test_loss\": test_loss,\n            }\n        )\n        self.log(lr, epoch, log_dict)\n        # saving checkpoint\n        state = {\n            \"epoch\": epoch,\n            \"model\": self.distiller.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"best_acc\": self.best_acc,\n        }\n        student_state = {\"model\": self.distiller.student.state_dict()}\n        save_checkpoint(state, os.path.join(self.log_path, \"latest\"))\n        save_checkpoint(\n            student_state, os.path.join(self.log_path, \"student_latest\")\n        )\n        if epoch % 20 == 0:\n            save_checkpoint(\n                state, os.path.join(self.log_path, \"epoch_{}\".format(epoch))\n            )\n            save_checkpoint(\n                student_state,\n                os.path.join(self.log_path, \"student_{}\".format(epoch)),\n            )\n        # update the best\n        if test_acc >= self.best_acc:\n            save_checkpoint(state, os.path.join(self.log_path, \"best\"))\n            save_checkpoint(\n                student_state, os.path.join(self.log_path, \"student_best\")\n            )\n\n    def train_iter(self, data, epoch, train_meters):\n        self.optimizer.zero_grad()\n        train_start_time = time.time()\n        image, target = data  # Adjusted to match the output of your data loader\n        train_meters[\"data_time\"].update(time.time() - train_start_time)\n        image = image.float()\n        image = image.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n    \n        # forward\n        preds, losses_dict = self.distiller(image=image, target=target, epoch=epoch)\n    \n        # backward\n        loss = sum([l.mean() for l in losses_dict.values()])\n        loss.backward()\n        self.optimizer.step()\n        train_meters[\"training_time\"].update(time.time() - train_start_time)\n        # collect info\n        batch_size = image.size(0)\n        acc1, acc5 = accuracy(preds, target, topk=(1, 5))\n        train_meters[\"losses\"].update(loss.cpu().detach().numpy().mean(), batch_size)\n        train_meters[\"top1\"].update(acc1[0], batch_size)\n        train_meters[\"top5\"].update(acc5[0], batch_size)\n        # print info\n        msg = \"Epoch:{}| Time(data):{:.3f}| Time(train):{:.3f}| Loss:{:.4f}| Top-1:{:.3f}| Top-5:{:.3f}\".format(\n            epoch,\n            train_meters[\"data_time\"].avg,\n            train_meters[\"training_time\"].avg,\n            train_meters[\"losses\"].avg,\n            train_meters[\"top1\"].avg,\n            train_meters[\"top5\"].avg,\n        )\n        return (msg, train_meters[\"losses\"].avg)\n        \nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef validate(val_loader, distiller):\n    batch_time, losses, top1, top5 = [AverageMeter() for _ in range(4)]\n    criterion = nn.CrossEntropyLoss()\n    num_iter = len(val_loader)\n    pbar = tqdm(range(num_iter))\n\n    distiller.eval()\n    with torch.no_grad():\n        start_time = time.time()\n        for idx, (image, target) in enumerate(val_loader):\n            image = image.float()\n            image = image.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n            output = distiller(image=image)\n            loss = criterion(output, target)\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            batch_size = image.size(0)\n            losses.update(loss.cpu().detach().numpy().mean(), batch_size)\n            top1.update(acc1[0], batch_size)\n            top5.update(acc5[0], batch_size)\n\n            # measure elapsed time\n            batch_time.update(time.time() - start_time)\n            start_time = time.time()\n            msg = \"Top-1:{top1.avg:.3f}| Top-5:{top5.avg:.3f}\".format(\n                top1=top1, top5=top5\n            )\n            pbar.set_description(log_msg(msg, \"EVAL\"))\n            pbar.update()\n    pbar.close()\n    return top1.avg, top5.avg, losses.avg\n\ndef log_msg(msg, mode=\"INFO\"):\n    color_map = {\n        \"INFO\": 36,\n        \"TRAIN\": 32,\n        \"EVAL\": 31,\n    }\n    msg = \"\\033[{}m[{}] {}\\033[0m\".format(color_map[mode], mode, msg)\n    return msg\n\ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\ndef save_checkpoint(obj, path):\n    with open(path, \"wb\") as f:\n        torch.save(obj, f)\n\ndef load_checkpoint(path):\n    with open(path, \"rb\") as f:\n        return torch.load(f, map_location=\"cpu\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-25T15:27:54.094304Z",
     "iopub.execute_input": "2025-01-25T15:27:54.094629Z",
     "iopub.status.idle": "2025-01-25T15:27:54.125566Z",
     "shell.execute_reply.started": "2025-01-25T15:27:54.094598Z",
     "shell.execute_reply": "2025-01-25T15:27:54.124753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-25T15:27:54.126230Z",
     "iopub.execute_input": "2025-01-25T15:27:54.126465Z",
     "iopub.status.idle": "2025-01-25T15:27:54.146801Z",
     "shell.execute_reply.started": "2025-01-25T15:27:54.126445Z",
     "shell.execute_reply": "2025-01-25T15:27:54.145947Z"
    },
    "papermill": {
     "duration": 0.023016,
     "end_time": "2024-12-28T15:03:24.769399",
     "exception": false,
     "start_time": "2024-12-28T15:03:24.746383",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-25T15:27:54.147686Z",
     "iopub.execute_input": "2025-01-25T15:27:54.147951Z",
     "iopub.status.idle": "2025-01-25T15:27:54.166865Z",
     "shell.execute_reply.started": "2025-01-25T15:27:54.147924Z",
     "shell.execute_reply": "2025-01-25T15:27:54.166177Z"
    },
    "papermill": {
     "duration": 0.020375,
     "end_time": "2024-12-28T15:03:24.792018",
     "exception": false,
     "start_time": "2024-12-28T15:03:24.771643",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-25T15:27:54.167541Z",
     "iopub.execute_input": "2025-01-25T15:27:54.167752Z",
     "iopub.status.idle": "2025-01-25T15:28:04.558780Z",
     "shell.execute_reply.started": "2025-01-25T15:27:54.167733Z",
     "shell.execute_reply": "2025-01-25T15:28:04.557761Z"
    },
    "papermill": {
     "duration": 7219.361104,
     "end_time": "2024-12-28T17:03:44.155369",
     "exception": false,
     "start_time": "2024-12-28T15:03:24.794265",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-25T15:28:04.562016Z",
     "iopub.execute_input": "2025-01-25T15:28:04.562423Z",
     "iopub.status.idle": "2025-01-25T16:40:15.570805Z",
     "shell.execute_reply.started": "2025-01-25T15:28:04.562380Z",
     "shell.execute_reply": "2025-01-25T16:40:15.570185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "student_model, path = cifar_model_dict[student]\nstudent_model = student_model(num_classes=num_classes)\nstudent_model.to(\"cuda\", non_blocking=True)\n\ndistiller = DTKD(student_model, teacher_model)\n\n# # Initialize the CRDTrainer\ntrainer = BaseTrainer(\n    experiment_name=\"DTKD\",\n    distiller=distiller,\n    train_loader=train_loader, \n    val_loader=val_loader\n)\n\ntrainer.train(num_epochs=max_epoch)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-25T16:40:15.571836Z",
     "iopub.execute_input": "2025-01-25T16:40:15.572077Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\ndef plot_losses():\n    # Extracting train and test losses for plotting\n    dtkd_train_loss = [entry['train_loss'] for entry in dtkd_losses]\n    dtkd_test_loss = [entry['test_loss'] for entry in dtkd_losses]\n    our_train_loss = [entry['train_loss'] for entry in our_losses]\n    our_test_loss = [entry['test_loss'] for entry in our_losses]\n    \n    # FOR 100 EPOCH\n    # Plotting\n    plt.figure(figsize=(8, 6)) \n    \n    # Train Losses\n    plt.subplot(2, 1, 1)  # Positioning in the first row\n    plt.plot(dtkd_train_loss, label=\"DTKD Train Loss\", color='blue')\n    plt.plot(our_train_loss, label=\"Our Train Loss\", color='red')\n    plt.title(\"Train Losses\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid()\n    \n    # Test Losses\n    plt.subplot(2, 1, 2)  # Positioning in the second row\n    plt.plot(dtkd_test_loss, label=\"DTKD Test Loss\", color='blue')\n    plt.plot(our_test_loss, label=\"Our Test Loss\", color='red')\n    plt.title(\"Test Losses\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid()\n    \n    plt.tight_layout()  # Adjust layout to avoid overlap\n    plt.show()\n\nplot_losses()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def plot_accuracies():\n    \n    # Extract data\n    dtkd_acc1 = [entry['acc@1'] for entry in dtkd_accuracies]\n    our_acc1 = [entry['acc@1'] for entry in our_accuracies]\n    \n    # Plotting\n    plt.figure(figsize=(8, 6))\n    plt.plot(dtkd_acc1, label=\"DTKD acc@1\", color='blue')\n    plt.plot(our_acc1, label=\"Our acc@1\", color='red')\n    \n    # Graph details\n    plt.title(\"Accuracy Comparison\", fontsize=16)\n    plt.xlabel(\"Epoch\", fontsize=12)\n    plt.ylabel(\"Accuracy (%)\", fontsize=12)\n    plt.legend()\n    plt.grid()\n    plt.tight_layout()\n    \n    # Show plot\n    plt.show()\n    \nplot_accuracies()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
